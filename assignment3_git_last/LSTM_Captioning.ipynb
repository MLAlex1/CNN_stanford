{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This mounts your Google Drive to the Colab VM.\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# TODO: Enter the foldername in your Drive where you have saved the unzipped\n",
    "# assignment folder, e.g. 'cs231n/assignments/assignment3/'\n",
    "FOLDERNAME = None\n",
    "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
    "\n",
    "# Now that we've mounted your Drive, this ensures that\n",
    "# the Python interpreter of the Colab VM can load\n",
    "# python files from within it.\n",
    "import sys\n",
    "sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n",
    "\n",
    "# This downloads the COCO dataset to your Drive\n",
    "# if it doesn't already exist.\n",
    "%cd /content/drive/My\\ Drive/$FOLDERNAME/cs231n/datasets/\n",
    "!bash get_datasets.sh\n",
    "%cd /content/drive/My\\ Drive/$FOLDERNAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Image Captioning with LSTMs\n",
    "In the previous exercise, you implemented a vanilla RNN and applied it to image captioning. In this notebook, you will implement the LSTM update rule and use it for image captioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "# Setup cell.\n",
    "import time, os, json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from cs231n.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
    "from cs231n.rnn_layers import *\n",
    "from cs231n.captioning_solver import CaptioningSolver\n",
    "from cs231n.classifiers.rnn import CaptioningRNN\n",
    "from cs231n.coco_utils import load_coco_data, sample_coco_minibatch, decode_captions\n",
    "from cs231n.image_utils import image_from_url\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # Set default size of plots.\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COCO Dataset\n",
    "As in the previous notebook, we will use the COCO dataset for captioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base dir  C:\\Users\\alexe\\Desktop\\stanford_CV\\assignment3_git\\cs231n\\datasets/coco_captioning\n",
      "train_captions <class 'numpy.ndarray'> (400135, 17) int32\n",
      "train_image_idxs <class 'numpy.ndarray'> (400135,) int32\n",
      "val_captions <class 'numpy.ndarray'> (195954, 17) int32\n",
      "val_image_idxs <class 'numpy.ndarray'> (195954,) int32\n",
      "train_features <class 'numpy.ndarray'> (82783, 512) float32\n",
      "val_features <class 'numpy.ndarray'> (40504, 512) float32\n",
      "idx_to_word <class 'list'> 1004\n",
      "word_to_idx <class 'dict'> 1004\n",
      "train_urls <class 'numpy.ndarray'> (82783,) <U63\n",
      "val_urls <class 'numpy.ndarray'> (40504,) <U63\n"
     ]
    }
   ],
   "source": [
    "# Load COCO data from disk into a dictionary.\n",
    "data = load_coco_data(pca_features=True)\n",
    "\n",
    "# Print out all the keys and values from the data dictionary.\n",
    "for k, v in data.items():\n",
    "    if type(v) == np.ndarray:\n",
    "        print(k, type(v), v.shape, v.dtype)\n",
    "    else:\n",
    "        print(k, type(v), len(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "A common variant on the vanilla RNN is the Long-Short Term Memory (LSTM) RNN. Vanilla RNNs can be tough to train on long sequences due to vanishing and exploding gradients caused by repeated matrix multiplication. LSTMs solve this problem by replacing the simple update rule of the vanilla RNN with a gating mechanism as follows.\n",
    "\n",
    "Similar to the vanilla RNN, at each timestep we receive an input $x_t\\in\\mathbb{R}^D$ and the previous hidden state $h_{t-1}\\in\\mathbb{R}^H$; the LSTM also maintains an $H$-dimensional *cell state*, so we also receive the previous cell state $c_{t-1}\\in\\mathbb{R}^H$. The learnable parameters of the LSTM are an *input-to-hidden* matrix $W_x\\in\\mathbb{R}^{4H\\times D}$, a *hidden-to-hidden* matrix $W_h\\in\\mathbb{R}^{4H\\times H}$ and a *bias vector* $b\\in\\mathbb{R}^{4H}$.\n",
    "\n",
    "At each timestep we first compute an *activation vector* $a\\in\\mathbb{R}^{4H}$ as $a=W_xx_t + W_hh_{t-1}+b$. We then divide this into four vectors $a_i,a_f,a_o,a_g\\in\\mathbb{R}^H$ where $a_i$ consists of the first $H$ elements of $a$, $a_f$ is the next $H$ elements of $a$, etc. We then compute the *input gate* $g\\in\\mathbb{R}^H$, *forget gate* $f\\in\\mathbb{R}^H$, *output gate* $o\\in\\mathbb{R}^H$ and *block input* $g\\in\\mathbb{R}^H$ as\n",
    "\n",
    "$$\n",
    "i = \\sigma(a_i) \\hspace{2pc}\n",
    "f = \\sigma(a_f) \\hspace{2pc}\n",
    "o = \\sigma(a_o) \\hspace{2pc}\n",
    "g = \\tanh(a_g)\n",
    "$$\n",
    "\n",
    "where $\\sigma$ is the sigmoid function and $\\tanh$ is the hyperbolic tangent, both applied elementwise.\n",
    "\n",
    "Finally we compute the next cell state $c_t$ and next hidden state $h_t$ as\n",
    "\n",
    "$$\n",
    "c_{t} = f\\odot c_{t-1} + i\\odot g \\hspace{4pc}\n",
    "h_t = o\\odot\\tanh(c_t)\n",
    "$$\n",
    "\n",
    "where $\\odot$ is the elementwise product of vectors.\n",
    "\n",
    "In the rest of the notebook we will implement the LSTM update rule and apply it to the image captioning task. \n",
    "\n",
    "In the code, we assume that data is stored in batches so that $X_t \\in \\mathbb{R}^{N\\times D}$ and will work with *transposed* versions of the parameters: $W_x \\in \\mathbb{R}^{D \\times 4H}$, $W_h \\in \\mathbb{R}^{H\\times 4H}$ so that activations $A \\in \\mathbb{R}^{N\\times 4H}$ can be computed efficiently as $A = X_t W_x + H_{t-1} W_h$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM: Step Forward\n",
    "Implement the forward pass for a single timestep of an LSTM in the `lstm_step_forward` function in the file `cs231n/rnn_layers.py`. This should be similar to the `rnn_step_forward` function that you implemented above, but using the LSTM update rule instead.\n",
    "\n",
    "Once you are done, run the following to perform a simple test of your implementation. You should see errors on the order of `e-8` or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_h error:  5.7054131967097955e-09\n",
      "next_c error:  5.8143123088804145e-09\n"
     ]
    }
   ],
   "source": [
    "N, D, H = 3, 4, 5\n",
    "x = np.linspace(-0.4, 1.2, num=N*D).reshape(N, D)\n",
    "prev_h = np.linspace(-0.3, 0.7, num=N*H).reshape(N, H)\n",
    "prev_c = np.linspace(-0.4, 0.9, num=N*H).reshape(N, H)\n",
    "Wx = np.linspace(-2.1, 1.3, num=4*D*H).reshape(D, 4 * H)\n",
    "Wh = np.linspace(-0.7, 2.2, num=4*H*H).reshape(H, 4 * H)\n",
    "b = np.linspace(0.3, 0.7, num=4*H)\n",
    "\n",
    "next_h, next_c, cache = lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)\n",
    "\n",
    "expected_next_h = np.asarray([\n",
    "    [ 0.24635157,  0.28610883,  0.32240467,  0.35525807,  0.38474904],\n",
    "    [ 0.49223563,  0.55611431,  0.61507696,  0.66844003,  0.7159181 ],\n",
    "    [ 0.56735664,  0.66310127,  0.74419266,  0.80889665,  0.858299  ]])\n",
    "expected_next_c = np.asarray([\n",
    "    [ 0.32986176,  0.39145139,  0.451556,    0.51014116,  0.56717407],\n",
    "    [ 0.66382255,  0.76674007,  0.87195994,  0.97902709,  1.08751345],\n",
    "    [ 0.74192008,  0.90592151,  1.07717006,  1.25120233,  1.42395676]])\n",
    "\n",
    "print('next_h error: ', rel_error(expected_next_h, next_h))\n",
    "print('next_c error: ', rel_error(expected_next_c, next_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM: Step Backward\n",
    "Implement the backward pass for a single LSTM timestep in the function `lstm_step_backward` in the file `cs231n/rnn_layers.py`. Once you are done, run the following to perform numeric gradient checking on your implementation. You should see errors on the order of `e-7` or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx error:  5.833434184302307e-10\n",
      "dh error:  3.4168728051126624e-10\n",
      "dc error:  1.5221747946070454e-10\n",
      "dWx error:  1.6933647290937225e-09\n",
      "dWh error:  2.7311389754207126e-08\n",
      "db error:  1.7349350966431763e-10\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "\n",
    "N, D, H = 4, 5, 6\n",
    "x = np.random.randn(N, D)\n",
    "prev_h = np.random.randn(N, H)\n",
    "prev_c = np.random.randn(N, H)\n",
    "Wx = np.random.randn(D, 4 * H)\n",
    "Wh = np.random.randn(H, 4 * H)\n",
    "b = np.random.randn(4 * H)\n",
    "\n",
    "next_h, next_c, cache = lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)\n",
    "\n",
    "dnext_h = np.random.randn(*next_h.shape)\n",
    "dnext_c = np.random.randn(*next_c.shape)\n",
    "\n",
    "fx_h = lambda x: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[0]\n",
    "fh_h = lambda h: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[0]\n",
    "fc_h = lambda c: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[0]\n",
    "fWx_h = lambda Wx: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[0]\n",
    "fWh_h = lambda Wh: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[0]\n",
    "fb_h = lambda b: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[0]\n",
    "\n",
    "fx_c = lambda x: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[1]\n",
    "fh_c = lambda h: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[1]\n",
    "fc_c = lambda c: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[1]\n",
    "fWx_c = lambda Wx: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[1]\n",
    "fWh_c = lambda Wh: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[1]\n",
    "fb_c = lambda b: lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b)[1]\n",
    "\n",
    "num_grad = eval_numerical_gradient_array\n",
    "\n",
    "dx_num = num_grad(fx_h, x, dnext_h) + num_grad(fx_c, x, dnext_c)\n",
    "dh_num = num_grad(fh_h, prev_h, dnext_h) + num_grad(fh_c, prev_h, dnext_c)\n",
    "dc_num = num_grad(fc_h, prev_c, dnext_h) + num_grad(fc_c, prev_c, dnext_c)\n",
    "dWx_num = num_grad(fWx_h, Wx, dnext_h) + num_grad(fWx_c, Wx, dnext_c)\n",
    "dWh_num = num_grad(fWh_h, Wh, dnext_h) + num_grad(fWh_c, Wh, dnext_c)\n",
    "db_num = num_grad(fb_h, b, dnext_h) + num_grad(fb_c, b, dnext_c)\n",
    "\n",
    "dx, dh, dc, dWx, dWh, db = lstm_step_backward(dnext_h, dnext_c, cache)\n",
    "\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dh error: ', rel_error(dh_num, dh))\n",
    "print('dc error: ', rel_error(dc_num, dc))\n",
    "print('dWx error: ', rel_error(dWx_num, dWx))\n",
    "print('dWh error: ', rel_error(dWh_num, dWh))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM: Forward\n",
    "In the function `lstm_forward` in the file `cs231n/rnn_layers.py`, implement the `lstm_forward` function to run an LSTM forward on an entire timeseries of data.\n",
    "\n",
    "When you are done, run the following to check your implementation. You should see an error on the order of `e-7` or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h error:  8.610537442272635e-08\n"
     ]
    }
   ],
   "source": [
    "N, D, H, T = 2, 5, 4, 3\n",
    "x = np.linspace(-0.4, 0.6, num=N*T*D).reshape(N, T, D)\n",
    "h0 = np.linspace(-0.4, 0.8, num=N*H).reshape(N, H)\n",
    "Wx = np.linspace(-0.2, 0.9, num=4*D*H).reshape(D, 4 * H)\n",
    "Wh = np.linspace(-0.3, 0.6, num=4*H*H).reshape(H, 4 * H)\n",
    "b = np.linspace(0.2, 0.7, num=4*H)\n",
    "\n",
    "h, cache = lstm_forward(x, h0, Wx, Wh, b)\n",
    "\n",
    "expected_h = np.asarray([\n",
    " [[ 0.01764008,  0.01823233,  0.01882671,  0.0194232 ],\n",
    "  [ 0.11287491,  0.12146228,  0.13018446,  0.13902939],\n",
    "  [ 0.31358768,  0.33338627,  0.35304453,  0.37250975]],\n",
    " [[ 0.45767879,  0.4761092,   0.4936887,   0.51041945],\n",
    "  [ 0.6704845,   0.69350089,  0.71486014,  0.7346449 ],\n",
    "  [ 0.81733511,  0.83677871,  0.85403753,  0.86935314]]])\n",
    "\n",
    "print('h error: ', rel_error(expected_h, h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM: Backward\n",
    "Implement the backward pass for an LSTM over an entire timeseries of data in the function `lstm_backward` in the file `cs231n/rnn_layers.py`. When you are done, run the following to perform numeric gradient checking on your implementation. You should see errors on the order of `e-8` or less. (For `dWh`, it's fine if your error is on the order of `e-6` or less)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx error:  7.148080829944732e-10\n",
      "dh0 error:  2.3824794284312267e-08\n",
      "dWx error:  8.000902497865783e-10\n",
      "dWh error:  6.064215130763785e-07\n",
      "db error:  6.280899116725197e-10\n"
     ]
    }
   ],
   "source": [
    "from cs231n.rnn_layers import lstm_forward, lstm_backward\n",
    "np.random.seed(231)\n",
    "\n",
    "N, D, T, H = 2, 3, 10, 6\n",
    "\n",
    "x = np.random.randn(N, T, D)\n",
    "h0 = np.random.randn(N, H)\n",
    "Wx = np.random.randn(D, 4 * H)\n",
    "Wh = np.random.randn(H, 4 * H)\n",
    "b = np.random.randn(4 * H)\n",
    "\n",
    "out, cache = lstm_forward(x, h0, Wx, Wh, b)\n",
    "\n",
    "dout = np.random.randn(*out.shape)\n",
    "\n",
    "dx, dh0, dWx, dWh, db = lstm_backward(dout, cache)\n",
    "\n",
    "fx = lambda x: lstm_forward(x, h0, Wx, Wh, b)[0]\n",
    "fh0 = lambda h0: lstm_forward(x, h0, Wx, Wh, b)[0]\n",
    "fWx = lambda Wx: lstm_forward(x, h0, Wx, Wh, b)[0]\n",
    "fWh = lambda Wh: lstm_forward(x, h0, Wx, Wh, b)[0]\n",
    "fb = lambda b: lstm_forward(x, h0, Wx, Wh, b)[0]\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(fx, x, dout)\n",
    "dh0_num = eval_numerical_gradient_array(fh0, h0, dout)\n",
    "dWx_num = eval_numerical_gradient_array(fWx, Wx, dout)\n",
    "dWh_num = eval_numerical_gradient_array(fWh, Wh, dout)\n",
    "db_num = eval_numerical_gradient_array(fb, b, dout)\n",
    "\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dh0 error: ', rel_error(dh0_num, dh0))\n",
    "print('dWx error: ', rel_error(dWx_num, dWx))\n",
    "print('dWh error: ', rel_error(dWh_num, dWh))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanishing gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "N, D, T, H = 1, 3, 3, 4\n",
    "\n",
    "x = np.random.randn(N, T, D)\n",
    "h0 = np.random.randn(N, H)\n",
    "Wx = np.random.randn(D, H)\n",
    "Wh = np.random.randn(H, H)\n",
    "b = np.random.randn(H)\n",
    "\n",
    "out, cache = rnn_forward(x, h0, Wx, Wh, b)\n",
    "\n",
    "dout = np.random.randn(*out.shape)\n",
    "\n",
    "dx, dh0, dWx, dWh, db = rnn_backward(dout, cache)\n",
    "\n",
    "dh = dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to understand vanishing gradient, as we described in the blog https://medium.com/@alexml0123/backpropagation-and-vanishing-gradient-problem-in-rnn-clearly-explained-efce8824971b,\n",
    "# we need to compute the contribution to individual losses at each timestep. The loop we used to compute the gradient in the tutorial doesn't directly provide us with this information,\n",
    "# thus we need to modify the backpropagation code to compute those components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first compute manually all the components of Loss3 with respect to Wx. \n",
    "# There will be 3 components because there are 3 timesteps up to and including Loss3\n",
    "\n",
    "t = 2\n",
    "(xt, _, _, next_h, prev_h) = cache[t]\n",
    "dLdWx_comp1 = xt.T.dot( dh[:, t, :]  * (1 - next_h**2))\n",
    "\n",
    "xt_1 = cache[t-1][0]\n",
    "next_h_1 = cache[t-1][-2]\n",
    "dLdWx_comp2 = xt_1.T.dot((dh[:, t, :]  * (1 - next_h**2)).dot(Wh.T) * (1 - next_h_1**2))\n",
    "\n",
    "xt_2 = cache[t-2][0]\n",
    "next_h_2 = cache[t-2][-2]\n",
    "dLdWx_comp3 = xt_2.T.dot(((dh[:, t, :] * (1 - next_h**2)).dot(Wh.T) * (1 - next_h_1**2)).dot(Wh.T) * (1 - next_h_2**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.11265219, -0.32975674,  1.04298019,  0.6696128 ],\n",
       "       [ 0.04854351,  0.13945587, -0.46157534, -0.29297507],\n",
       "       [-0.01994111, -0.04071621,  0.22437918,  0.12942807]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dLdWx_comp1 + dLdWx_comp2 + dLdWx_comp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.12964021, -0.36447594,  1.01880983,  0.68256384],\n",
       "       [ 0.05655798,  0.15900947, -0.44447492, -0.2977813 ],\n",
       "       [-0.02370473, -0.06664448,  0.18628953,  0.1248069 ]])"
      ]
     },
     "execution_count": 863,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dLdWx_comp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.12964824, -0.36447594,  1.01880983,  0.68256384],\n",
       "       [ 0.05656148,  0.15900947, -0.44447492, -0.2977813 ],\n",
       "       [-0.0237062 , -0.06664448,  0.18628953,  0.1248069 ]])"
      ]
     },
     "execution_count": 680,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # to evaluate the gradient of L3 with respect to x3\n",
    "# fWx = lambda Wx: rnn_step_forward(x[:, -1, :], next_h_1, Wx, Wh, b)[0]\n",
    "# eval_numerical_gradient_array(fWx, Wx, dout[:, 2, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.12964021, -0.36447594,  1.01880983,  0.68256384],\n",
       "       [ 0.05655798,  0.15900947, -0.44447492, -0.2977813 ],\n",
       "       [-0.02370473, -0.06664448,  0.18628953,  0.1248069 ]])"
      ]
     },
     "execution_count": 862,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to evaluate the gradient of L3 with respect to x3\n",
    "fWx = lambda Wx: rnn_forward(x[:, -1:, :], next_h_1, Wx, Wh, b)[0][:, -1, :]\n",
    "eval_numerical_gradient_array(fWx, Wx, dout[:, 2, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.11265219, -0.32975674,  1.04298019,  0.6696128 ],\n",
       "       [ 0.04854351,  0.13945587, -0.46157534, -0.29297507],\n",
       "       [-0.01994111, -0.04071621,  0.22437918,  0.12942807]])"
      ]
     },
     "execution_count": 854,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How L3 (or more precisely, last hidden state (we select [:, -1, :]) as\n",
    "# we compute h instead of losses here assuming the upstream gradient is known - dout)\n",
    "# changes if we change Wx. However, we cannot get individual effect in this way, only the entire\n",
    "# sum, however we can do it as difference. See below\n",
    "fWx = lambda Wx: rnn_forward(x, h0, Wx, Wh, b)[0][:, -1, :]\n",
    "L3_tot = eval_numerical_gradient_array(fWx, Wx, dout[:, -1, :])\n",
    "L3_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.12588038, -0.3494092 ,  1.03741126,  0.68272983],\n",
       "       [ 0.05352547,  0.14685732, -0.45947799, -0.29791517],\n",
       "       [-0.01563983, -0.03432602,  0.22618997,  0.12516294]])"
      ]
     },
     "execution_count": 672,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dLdWx_comp2 + dLdWx_comp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.12588861, -0.3494091 ,  1.03741094,  0.68272982],\n",
       "       [ 0.05352913,  0.14685725, -0.45947774, -0.29791518],\n",
       "       [-0.0156417 , -0.03432582,  0.22618932,  0.125163  ]])"
      ]
     },
     "execution_count": 671,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the last two sums, i.e., effect of the change in Wx starting from x2 (x2 and x3)\n",
    "# we then can subtract this effect from total L3 effect to get effect for x1 only\n",
    "fWx = lambda Wx: rnn_forward(x[:, 1:, :], next_h_2, Wx, Wh, b)[0][:, -1, :]\n",
    "eval_numerical_gradient_array(fWx, Wx, dout[:, -1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01322724,  0.01965264,  0.00556924, -0.01311809],\n",
       "       [-0.00498157, -0.0074015 , -0.00209763,  0.00494056],\n",
       "       [-0.00430122, -0.0063904 , -0.00180996,  0.00426494]])"
      ]
     },
     "execution_count": 683,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the last two sums, i.e., effect of the change in Wx starting from x2 (x2 and x3)\n",
    "# we then can subtract this effect from total L3 effect to get effect for x1 only\n",
    "fWx = lambda Wx: rnn_forward(x[:, 1:, :], next_h_2, Wx, Wh, b)[0][:, -1, :]\n",
    "L3_tot - eval_numerical_gradient_array(fWx, Wx, dout[:, -1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0132282 ,  0.01965245,  0.00556892, -0.01311703],\n",
       "       [-0.00498197, -0.00740145, -0.00209735,  0.0049401 ],\n",
       "       [-0.00430128, -0.00639019, -0.00181079,  0.00426513]])"
      ]
     },
     "execution_count": 674,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dLdWx_comp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00375982,  0.01506674,  0.01860143,  0.00016598],\n",
       "       [-0.0030325 , -0.01215215, -0.01500307, -0.00013388],\n",
       "       [ 0.0080649 ,  0.03231846,  0.03990044,  0.00035604]])"
      ]
     },
     "execution_count": 861,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the middle component, we simply subtract from sum of two components (dLdWx_comp2 + dLdWx_comp1) the first component\n",
    "fWx1 = lambda Wx: rnn_forward(x[:, -1:, :], next_h_1, Wx, Wh, b)[0][:, -1, :] # first component\n",
    "fWx3 = lambda Wx: rnn_forward(x[:, 1:, :], next_h_2, Wx, Wh, b)[0][:, -1, :] # sum of first two components\n",
    "# L3_tot - eval_numerical_gradient_array(fWx1, Wx, dout[:, -1, :]) - (L3_tot - eval_numerical_gradient_array(fWx3, Wx, dout[:, -1, :]))\n",
    "# same as\n",
    "eval_numerical_gradient_array(fWx3, Wx, dout[:, -1, :]) - eval_numerical_gradient_array(fWx1, Wx, dout[:, -1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00375982,  0.01506674,  0.01860143,  0.00016598],\n",
       "       [-0.0030325 , -0.01215215, -0.01500307, -0.00013388],\n",
       "       [ 0.0080649 ,  0.03231846,  0.03990044,  0.00035604]])"
      ]
     },
     "execution_count": 856,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dLdWx_comp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.11265219, -0.32975674,  1.04298019,  0.6696128 ],\n",
       "       [ 0.04854351,  0.13945587, -0.46157534, -0.29297507],\n",
       "       [-0.01994111, -0.04071621,  0.22437918,  0.12942807]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this is just for comparison from previous implementation\n",
    "\n",
    "t = 2\n",
    "# (xt, prev_h, _, _, _, next_h) = cache[t]\n",
    "(xt, _, _, next_h, prev_h) = cache[t]\n",
    "dJ3dWxh =            xt.T.dot( dh[:, t, :]  * (1 - cache[t][-2]**2)) + \\\n",
    "          cache[t-1][0].T.dot((dh[:, t, :]  * (1 - cache[t][-2]**2)).dot(Wh.T) * (1 - cache[t-1][-2]**2)) + \\\n",
    "          cache[t-2][0].T.dot(((dh[:, t, :] * (1 - cache[t][-2]**2)).dot(Wh.T) * (1 - cache[t-1][-2]**2)).dot(Wh.T) * (1 - cache[t-2][-2]**2))\n",
    "display(dJ3dWxh)\n",
    "# t = 1\n",
    "# (xt, _, _, next_h, prev_h)  = cache[t]\n",
    "# dJ2dWxh =            xt.T.dot( dh[:, t, :] * (1 - cache[t][-2]**2)) + \\\n",
    "#           cache[t-1][0].T.dot((dh[:, t, :] * (1 - cache[t][-2]**2)).dot(Wh.T) * (1 - cache[t-1][-2]**2))\n",
    "\n",
    "# t = 0\n",
    "# (xt, _, _, next_h, prev_h)  = cache[t]\n",
    "# dJ1dWxh =            xt.T.dot( dh[:, t, :] * (1 - cache[t][-2]**2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's do it automatically for all timesteps and all losees \n",
    "\n",
    "# in the losses dict, first key represent the timestep of the loss, while keys within each loss\n",
    "# represent the gradient for each component\n",
    "losses = {i : {x_comp : 0 for x_comp in range(i)} for i in range(T)}\n",
    "dh = dout\n",
    "for i in range(T-1, -1, -1):\n",
    "    mid =  dh[:, i, :]  * (1 - cache[i][-2]**2)\n",
    "    for j in range(i, -1, -1):\n",
    "        xj = cache[j][0]\n",
    "        next_h_j = cache[j-1][-2]\n",
    "        losses[i][j] = xj.T.dot(mid)\n",
    "        mid = mid.dot(Wh.T) * (1 - next_h_j**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.12964021, -0.36447594,  1.01880983,  0.68256384],\n",
       "        [ 0.05655798,  0.15900947, -0.44447492, -0.2977813 ],\n",
       "        [-0.02370473, -0.06664448,  0.18628953,  0.1248069 ]]),\n",
       " array([[ 0.00375982,  0.01506674,  0.01860143,  0.00016598],\n",
       "        [-0.0030325 , -0.01215215, -0.01500307, -0.00013388],\n",
       "        [ 0.0080649 ,  0.03231846,  0.03990044,  0.00035604]]),\n",
       " array([[ 0.0132282 ,  0.01965245,  0.00556892, -0.01311703],\n",
       "        [-0.00498197, -0.00740145, -0.00209735,  0.0049401 ],\n",
       "        [-0.00430128, -0.00639019, -0.00181079,  0.00426513]]))"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dLdWx_comp1, dLdWx_comp2, dLdWx_comp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: array([[ 0.0132282 ,  0.01965245,  0.00556892, -0.01311703],\n",
       "        [-0.00498197, -0.00740145, -0.00209735,  0.0049401 ],\n",
       "        [-0.00430128, -0.00639019, -0.00181079,  0.00426513]]),\n",
       " 1: array([[ 0.00375982,  0.01506674,  0.01860143,  0.00016598],\n",
       "        [-0.0030325 , -0.01215215, -0.01500307, -0.00013388],\n",
       "        [ 0.0080649 ,  0.03231846,  0.03990044,  0.00035604]]),\n",
       " 2: array([[-0.12964021, -0.36447594,  1.01880983,  0.68256384],\n",
       "        [ 0.05655798,  0.15900947, -0.44447492, -0.2977813 ],\n",
       "        [-0.02370473, -0.06664448,  0.18628953,  0.1248069 ]])}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(losses[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.11265219, -0.32975674,  1.04298019,  0.6696128 ],\n",
       "       [ 0.04854351,  0.13945587, -0.46157534, -0.29297507],\n",
       "       [-0.01994111, -0.04071621,  0.22437918,  0.12942807]])"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dJ3dWxh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.11265219, -0.32975674,  1.04298019,  0.6696128 ],\n",
       "       [ 0.04854351,  0.13945587, -0.46157534, -0.29297507],\n",
       "       [-0.01994111, -0.04071621,  0.22437918,  0.12942807]])"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum([losses[2][e] for e in losses[2]], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can also see that if we sum all the losses across all components we get total gradient for dWx\n",
    "np.allclose(dWx, np.sum([losses[l][e] for l in losses for e in losses[l]], 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now looking at the components you should see that component_3, that corresponds to the loss update due to the x3 of this formula\n",
    "\\\n",
    "\\\n",
    "$$\\frac{\\partial L_3}{\\partial W_{xh}} = dout_3\\times(\\frac{\\partial h_3}{\\partial z_{3}}\n",
    "\\frac{\\partial z_3}{\\partial W_{xh}} +\\frac{\\partial h_3}{\\partial z_{3}}\n",
    "\\frac{\\partial z_3}{\\partial h_2}\\frac{\\partial h_2}{\\partial z_2} \\frac{\\partial z_2}{\\partial W_{xh}} +\\frac{\\partial h_3}{\\partial z_{3}}\\frac{\\partial z_3}{\\partial h_2}\\frac{\\partial h_2}{\\partial z_2} \\frac{\\partial z_2}{\\partial h_1}\\frac{\\partial h_1}{\\partial z_1}\\frac{\\partial z_1}{\\partial W_{xh}})$$\n",
    "\n",
    "so this part \n",
    "$$\\frac{\\partial h_3}{\\partial z_{3}}\\frac{\\partial z_3}{\\partial h_2}\\frac{\\partial h_2}{\\partial z_2} \\frac{\\partial z_2}{\\partial h_1}\\frac{\\partial h_1}{\\partial z_1}\\frac{\\partial z_1}{\\partial W_{xh}}$$\n",
    "\\\n",
    "which basically says how much the input in x3 contributes to the L3, we see that this component corresponds to the smallest gradient thus there is not much info in L3 about this part and this is the vanishing gradient problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.12964021 -0.36447594  1.01880983  0.68256384]\n",
      " [ 0.05655798  0.15900947 -0.44447492 -0.2977813 ]\n",
      " [-0.02370473 -0.06664448  0.18628953  0.1248069 ]] \n",
      "\n",
      " [[ 0.00375982  0.01506674  0.01860143  0.00016598]\n",
      " [-0.0030325  -0.01215215 -0.01500307 -0.00013388]\n",
      " [ 0.0080649   0.03231846  0.03990044  0.00035604]] \n",
      "\n",
      " [[ 0.0132282   0.01965245  0.00556892 -0.01311703]\n",
      " [-0.00498197 -0.00740145 -0.00209735  0.0049401 ]\n",
      " [-0.00430128 -0.00639019 -0.00181079  0.00426513]]\n",
      "\n",
      "\n",
      "Let's see the magnitudes of them\n",
      "\n",
      " comp 1: 1.4225029296044476 \n",
      " comp 2: 0.06058764108583098 \n",
      " comp 3: 0.03087853793900866\n"
     ]
    }
   ],
   "source": [
    "print(dLdWx_comp1, \"\\n\\n\", dLdWx_comp2, \"\\n\\n\", dLdWx_comp3)\n",
    "print(\"\\n\")\n",
    "print(\"Let's see the magnitudes of them\")\n",
    "print(\"\\n comp 1:\", np.linalg.norm(dLdWx_comp1), \"\\n comp 2:\", np.linalg.norm(dLdWx_comp2),\"\\n comp 3:\", np.linalg.norm(dLdWx_comp3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.0132282 ,  0.01965245,  0.00556892, -0.01311703],\n",
       "        [-0.00498197, -0.00740145, -0.00209735,  0.0049401 ],\n",
       "        [-0.00430128, -0.00639019, -0.00181079,  0.00426513]]),\n",
       " array([[ 0.00375982,  0.01506674,  0.01860143,  0.00016598],\n",
       "        [-0.0030325 , -0.01215215, -0.01500307, -0.00013388],\n",
       "        [ 0.0080649 ,  0.03231846,  0.03990044,  0.00035604]]),\n",
       " array([[-0.12964021, -0.36447594,  1.01880983,  0.68256384],\n",
       "        [ 0.05655798,  0.15900947, -0.44447492, -0.2977813 ],\n",
       "        [-0.02370473, -0.06664448,  0.18628953,  0.1248069 ]])]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Let's see the magnitudes of them:\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['component 1 : 0.03087853793900866',\n",
       " 'component 2 : 0.06058764108583098',\n",
       " 'component 3 : 1.4225029296044476']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display([losses[2][e] for e in losses[2]])\n",
    "print(\"\\n\")\n",
    "print(\"Let's see the magnitudes of them:\")\n",
    "print(\"\\n\")\n",
    "display([f\"component {e+1} : {np.linalg.norm(losses[2][e])}\" for e in losses[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: array([[ 0.0132282 ,  0.01965245,  0.00556892, -0.01311703],\n",
       "        [-0.00498197, -0.00740145, -0.00209735,  0.0049401 ],\n",
       "        [-0.00430128, -0.00639019, -0.00181079,  0.00426513]]),\n",
       " 1: array([[ 0.00375982,  0.01506674,  0.01860143,  0.00016598],\n",
       "        [-0.0030325 , -0.01215215, -0.01500307, -0.00013388],\n",
       "        [ 0.0080649 ,  0.03231846,  0.03990044,  0.00035604]]),\n",
       " 2: array([[-0.12964021, -0.36447594,  1.01880983,  0.68256384],\n",
       "        [ 0.05655798,  0.15900947, -0.44447492, -0.2977813 ],\n",
       "        [-0.02370473, -0.06664448,  0.18628953,  0.1248069 ]])}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1073,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's try to see how the gradient vanishes with a longer sequence which is more clear\n",
    "np.random.seed(4)\n",
    "\n",
    "N, D, T, H = 1, 3, 100, 4\n",
    "\n",
    "x = np.random.randn(N, T, D)\n",
    "h0 = np.random.randn(N, H)\n",
    "Wx = np.random.randn(D, H)\n",
    "Wh = np.random.randn(H, H)\n",
    "b = np.random.randn(H)\n",
    "\n",
    "out, cache = rnn_forward(x, h0, Wx, Wh, b)\n",
    "\n",
    "dout = np.random.randn(*out.shape)\n",
    "\n",
    "dx, dh0, dWx, dWh, db = rnn_backward(dout, cache)\n",
    "\n",
    "dh = dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1043,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {i : {x_comp : 0 for x_comp in range(i)} for i in range(T)}\n",
    "dh = dout\n",
    "for i in range(T-1, -1, -1):\n",
    "    mid =  dh[:, i, :]  * (1 - cache[i][-2]**2)\n",
    "    for j in range(i, -1, -1):\n",
    "        xj = cache[j][0]\n",
    "        next_h_j = cache[j-1][-2]\n",
    "        losses[i][j] = xj.T.dot(mid)\n",
    "        mid = mid.dot(Wh.T) * (1 - next_h_j**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1044,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.18509249 -0.55226125  0.73607008 -0.1122698 ]\n",
      " [-0.05298749 -0.15809899  0.210719   -0.03214012]\n",
      " [ 0.10490278  0.31299887 -0.41717412  0.06362988]]\n",
      "\n",
      "[[ 6.58984104e-05  9.19971355e-09  2.53077815e-04  5.69596285e-04]\n",
      " [-2.48184752e-05 -3.46477042e-09 -9.53134598e-05 -2.14519762e-04]\n",
      " [-2.14275114e-05 -2.99137667e-09 -8.22907221e-05 -1.85209793e-04]]\n"
     ]
    }
   ],
   "source": [
    "# gradient related to the 9th token in the sequence for L9 update\n",
    "print(losses[9][9])\n",
    "print()\n",
    "# gradient related to the 1th token in the sequence for L9 update\n",
    "print(losses[9][0])\n",
    "# we can see the difference in weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1045,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0\n",
      "component 0 - 2.639022702589412\n",
      "Loss 1\n",
      "component 0 - 0.35851466255055137\n",
      "component 1 - 0.6074057155901502\n",
      "Loss 2\n",
      "component 0 - 0.02246250863922291\n",
      "component 1 - 0.03794331089931439\n",
      "component 2 - 0.4170928808476889\n",
      "Loss 3\n",
      "component 0 - 0.015912679217785775\n",
      "component 1 - 0.026774443897798098\n",
      "component 2 - 0.0812330329579129\n",
      "component 3 - 0.40399673180179607\n",
      "Loss 4\n",
      "component 0 - 0.006651069407462185\n",
      "component 1 - 0.011191304371251974\n",
      "component 2 - 0.0337329969291501\n",
      "component 3 - 0.16742842633403504\n",
      "component 4 - 0.2157415490612246\n",
      "Loss 5\n",
      "component 0 - 0.017857722587679443\n",
      "component 1 - 0.030055903361444685\n",
      "component 2 - 0.08545788785343011\n",
      "component 3 - 0.4284615578764166\n",
      "component 4 - 0.3509073457457624\n",
      "component 5 - 0.4491648235401462\n",
      "Loss 6\n",
      "component 0 - 0.0018401152289203558\n",
      "component 1 - 0.0031072842757313723\n",
      "component 2 - 0.018539407751486263\n",
      "component 3 - 0.0876292252190969\n",
      "component 4 - 0.05637453750209233\n",
      "component 5 - 0.07716675807383126\n",
      "component 6 - 0.1261672475031838\n",
      "Loss 7\n",
      "component 0 - 0.00023017160244774175\n",
      "component 1 - 0.0003871297883553918\n",
      "component 2 - 0.0015376719376008023\n",
      "component 3 - 0.007393219969132659\n",
      "component 4 - 0.005273755752973523\n",
      "component 5 - 0.0055443844872087285\n",
      "component 6 - 0.012725235282902106\n",
      "component 7 - 0.14107292437920455\n",
      "Loss 8\n",
      "component 0 - 0.0005225856406461387\n",
      "component 1 - 0.0008829203472447947\n",
      "component 2 - 0.005393431616938242\n",
      "component 3 - 0.025483425344056173\n",
      "component 4 - 0.01635972897687925\n",
      "component 5 - 0.022994937584405337\n",
      "component 6 - 0.03990539762983594\n",
      "component 7 - 0.39212342586525284\n",
      "component 8 - 0.9232163797427847\n",
      "Loss 9\n",
      "component 0 - 0.000700059867251646\n",
      "component 1 - 0.0011862246976535764\n",
      "component 2 - 0.008042327917238487\n",
      "component 3 - 0.03795652882545342\n",
      "component 4 - 0.02425203663429888\n",
      "component 5 - 0.03748391085306813\n",
      "component 6 - 0.059678883859013154\n",
      "component 7 - 0.587586869917645\n",
      "component 8 - 1.5169256008909238\n",
      "component 9 - 1.11979896940586\n"
     ]
    }
   ],
   "source": [
    "for l in losses:\n",
    "    print(f\"Loss {l}\")\n",
    "    for ii, e in losses[l].items():\n",
    "        print(f\"component {ii} - {np.linalg.norm(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1079,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1079,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(dWx, np.sum([losses[l][e] for l in losses for e in losses[l]], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1080,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16.73630427, 11.71855786, -6.01988139, -6.1078173 ],\n",
       "       [10.92607916, 18.57099391,  1.51201247, 10.39514205],\n",
       "       [ 9.92566987,  9.48630516, -1.11423128,  3.16016163]])"
      ]
     },
     "execution_count": 1080,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total gradients doesn't go to zero, differently from what he says in this article \n",
    "# https://medium.datadriveninvestor.com/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577\n",
    "dWx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # THIS IS WRONG - IGNORE\n",
    "\n",
    "# # this computes single components, so in the image\n",
    "# # on blog https://medium.com/@alexml0123/backpropagation-and-vanishing-gradient-problem-in-rnn-clearly-explained-efce8824971b\n",
    "# # lis[-1] corresponds to dout3 in the last row, so that single component of the sum\n",
    "# lis = {i : 0 for i in range(T)}\n",
    "# dh = dout\n",
    "# for i in range(T-1, -1, -1):\n",
    "#     mid =  dh[:, i, :]  * (1 - cache[i][-2]**2)\n",
    "#     for j in range(i, -1, -1):\n",
    "#         xj = cache[j][0]\n",
    "#         next_h_j = cache[j-1][-2]\n",
    "#         lis[i] += xj.T.dot(mid)\n",
    "#         mid = mid.dot(Wh.T) * (1 - next_h_j**2)\n",
    "\n",
    "# THIS MIGHT BE USEFUL - it computes results like in the image below in the blog\n",
    "# # this computes dWx for each loop like in the image on blog \n",
    "# # https://medium.com/@alexml0123/backpropagation-and-vanishing-gradient-problem-in-rnn-clearly-explained-efce8824971b\n",
    "# ls = {}\n",
    "# dh = dout\n",
    "# ht_next = 0\n",
    "# for i in range(len(cache)-1, -1, -1):\n",
    "#     ht_next += dh[:, i, :]\n",
    "#     l = cache[i][0].T.dot( ht_next  * (1 - cache[i][-2]**2))\n",
    "#     ls[i] = l\n",
    "#     ht_next = (ht_next  * (1 - cache[i][-2]**2)).dot(Wh.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1000,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "N, D, T, H = 1, 3, 3, 1\n",
    "\n",
    "x = np.random.randn(N, T, D)\n",
    "h0 = np.random.randn(N, H)\n",
    "Wx = np.random.randn(D, 4 * H)\n",
    "Wh = np.random.randn(H, 4 * H)\n",
    "b = np.random.randn(4 * H)\n",
    "\n",
    "out, cache = lstm_forward(x, h0, Wx, Wh, b)\n",
    "\n",
    "dout = np.random.randn(*out.shape)\n",
    "\n",
    "dx, dh0, dWx, dWh, db = lstm_backward(dout, cache)\n",
    "    \n",
    "dh = dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (x, next_h, prev_h, prev_c, Wx, Wh, next_h, next_c_t, i, f, o ,g) = cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1001,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_h_2 = cache[2][2]\n",
    "# differently from RNN, here we need to pass next_c as well. Normally you wouldn't need to do\n",
    "# it as you would always start from step 1, but here we did that modification to lstm_forward\n",
    "# to make it work for us\n",
    "next_c_2 = cache[2][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1002,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02349287,  0.00135057, -0.11156069, -0.05284914],\n",
       "       [ 0.01024921, -0.00058921,  0.04867045,  0.02305643],\n",
       "       [-0.00429567,  0.00024695, -0.02039889, -0.00966347]])"
      ]
     },
     "execution_count": 1002,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fWx = lambda Wx: lstm_forward(x[:, -1:, :], next_h_2, Wx, Wh, b, next_c=next_c_2)[0][:, -1, :]\n",
    "eval_numerical_gradient_array(fWx, Wx, dout[:, 2, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1003,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dgradmtx(h, dnext_c, dnext_h, prev_c, next_c_t, i, f, o, g):\n",
    "    dgrad = np.zeros((h.shape[0], 4 * h.shape[1]))\n",
    "    assert dgrad.shape[1] % 4 == 0\n",
    "    H = dgrad.shape[1] // 4\n",
    "    \n",
    "    # compute gradients wrt ai, af, ao and ag from two flows - next_h and next_c\n",
    "    dnextc_dai = dnext_c * (i * (1-i)) * g\n",
    "    dnextc_daf = dnext_c * (f * (1-f)) * prev_c\n",
    "    dnextc_dao = 0\n",
    "    dnextc_dag = dnext_c * (1 - g**2) * i\n",
    "\n",
    "    dnexth_dai = dnext_h * o * (1 - next_c_t**2) * (i * (1-i)) * g\n",
    "    dnexth_daf = dnext_h * o * (1 - next_c_t**2) * (f * (1-f)) * prev_c\n",
    "    dnexth_dao = dnext_h * (o * (1-o) * next_c_t)\n",
    "    dnexth_dag = dnext_h * o * (1 - next_c_t**2) * (1 - g**2) * i\n",
    "\n",
    "    # join them together in a matrix at this point to conveniently compute\n",
    "    # downstream gradients \n",
    "    dgrad[:, 0:H] = dnextc_dai + dnexth_dai\n",
    "    dgrad[:, H:2*H] = dnextc_daf + dnexth_daf\n",
    "    dgrad[:, 2*H:3*H] = dnextc_dao + dnexth_dao\n",
    "    dgrad[:, 3*H:4*H] = dnextc_dag + dnexth_dag\n",
    "    \n",
    "    dprev_c = dnext_c * f + dnext_h * o * (1 - next_c_t**2) * f\n",
    "    dprev_h = dgrad @ Wh.T\n",
    "    return dgrad, dprev_h, dprev_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1004,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnext_c = np.zeros((h0.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1005,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02349287,  0.00135057, -0.11156069, -0.05284914],\n",
       "       [ 0.01024921, -0.00058921,  0.04867045,  0.02305643],\n",
       "       [-0.00429567,  0.00024695, -0.02039889, -0.00966347]])"
      ]
     },
     "execution_count": 1005,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first component\n",
    "dgrad, _, _ = dgradmtx(h0, dnext_c, dout[:, 2, :], cache[2][3], cache[2][-5], cache[2][-4],  cache[2][-3], cache[2][-2], cache[2][-1])      \n",
    "cache[2][0].T @ dgrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1006,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_h_2 = cache[2][2]\n",
    "next_c_2 = cache[2][3]\n",
    "\n",
    "next_h_1 = cache[1][2]\n",
    "next_c_1 = cache[1][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1007,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-9.83990139e-03,  6.78775166e-05, -1.10660923e-03,\n",
       "         4.20773125e-04],\n",
       "       [ 7.93641636e-03, -5.47469144e-05,  8.92540614e-04,\n",
       "        -3.39376441e-04],\n",
       "       [-2.11067811e-02,  1.45598600e-04, -2.37369846e-03,\n",
       "         9.02566589e-04]])"
      ]
     },
     "execution_count": 1007,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the middle component, we simply subtract from sum of two components (dLdWx_comp2 + dLdWx_comp1) the first component\n",
    "fWx1 = lambda Wx: lstm_forward(x[:, -1:, :], next_h_2, Wx, Wh, b, next_c=next_c_2)[0][:, -1, :] # first component\n",
    "fWx3 = lambda Wx: lstm_forward(x[:, 1:, :], next_h_1, Wx, Wh, b, next_c=next_c_1)[0][:, -1, :] # sum of first two components\n",
    "eval_numerical_gradient_array(fWx3, Wx, dout[:, -1, :]) - eval_numerical_gradient_array(fWx1, Wx, dout[:, -1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1008,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-9.83990139e-03,  6.78775168e-05, -1.10660923e-03,\n",
       "         4.20773125e-04],\n",
       "       [ 7.93641636e-03, -5.47469140e-05,  8.92540613e-04,\n",
       "        -3.39376441e-04],\n",
       "       [-2.11067811e-02,  1.45598602e-04, -2.37369846e-03,\n",
       "         9.02566589e-04]])"
      ]
     },
     "execution_count": 1008,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# second component\n",
    "dnext_c = np.zeros((h0.shape))\n",
    "dgrad1, dprev_h1, dnext_c1 = dgradmtx(h0, dnext_c, dout[:, -1, :], cache[2][3], cache[2][-5], cache[2][-4],  cache[2][-3], cache[2][-2], cache[2][-1])    \n",
    "dgrad2, dprev_h2, dnext_c2 = dgradmtx(h0, dnext_c1, dprev_h1, cache[1][3], cache[1][-5], cache[1][-4],  cache[1][-3], cache[1][-2], cache[1][-1])      \n",
    "cache[1][0].T @ dgrad2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1009,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last component\n",
    "dnext_c = np.zeros((h0.shape))\n",
    "dgrad1, dprev_h1, dnext_c1 = dgradmtx(h0, dnext_c, dout[:, -1, :], cache[2][3], cache[2][-5], cache[2][-4],  cache[2][-3], cache[2][-2], cache[2][-1])    \n",
    "dgrad2, dprev_h2, dnext_c2 = dgradmtx(h0, dnext_c1, dprev_h1, cache[1][3], cache[1][-5], cache[1][-4],  cache[1][-3], cache[1][-2], cache[1][-1])      \n",
    "dgrad3, dprev_h3, dnext_c3 = dgradmtx(h0, dnext_c2, dprev_h2, cache[0][3], cache[0][-5], cache[0][-4],  cache[0][-3], cache[0][-2], cache[0][-1])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1010,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.95768947e-05, -1.71179260e-13,  2.77411351e-05,\n",
       "        -9.76467796e-03],\n",
       "       [ 7.37299513e-06, -2.43064032e-13, -1.04477890e-05,\n",
       "         3.67754574e-03],\n",
       "       [ 6.36562140e-06, -1.83183584e-12, -9.02030090e-06,\n",
       "         3.17508036e-03]])"
      ]
     },
     "execution_count": 1010,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fWx = lambda Wx: lstm_forward(x, h0, Wx, Wh, b)[0][:, -1, :]\n",
    "L3_tot = eval_numerical_gradient_array(fWx, Wx, dout[:, -1, :])\n",
    "L3_tot - cache[1][0].T @ dgrad2 - cache[2][0].T @ dgrad1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1016,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.95768961e-05,  0.00000000e+00,  2.77411349e-05,\n",
       "        -9.76467796e-03],\n",
       "       [ 7.37299593e-06,  0.00000000e+00, -1.04477887e-05,\n",
       "         3.67754574e-03],\n",
       "       [ 6.36561888e-06,  0.00000000e+00, -9.02030083e-06,\n",
       "         3.17508036e-03]])"
      ]
     },
     "execution_count": 1016,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache[0][0].T @ dgrad3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 992,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.allclose(L3_tot - cache[1][0].T @ dgrad2 - cache[2][0].T @ dgrad1, cache[0][0].T @ dgrad3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1019,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03335235,  0.00141845, -0.11263956, -0.06219304],\n",
       "       [ 0.01819299, -0.00064396,  0.04955254,  0.0263946 ],\n",
       "       [-0.02539609,  0.00039255, -0.02278161, -0.00558583]])"
      ]
     },
     "execution_count": 1019,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L3_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1033,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.allclose(np.sum([losses[2][e] for e in losses[2]], 0), L3_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1023,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {i : {x_comp : 0 for x_comp in range(i)} for i in range(T)}\n",
    "for i in range(T-1, -1, -1):\n",
    "    dnext_c = np.zeros((h0.shape))\n",
    "    dprev_h =  dout[:, i, :]\n",
    "    for j in range(i, -1, -1):\n",
    "        # def dgradmtx(h, dnext_c, dnext_h, prev_c, next_c_t, i, f, o, g)\n",
    "        dgrad, dprev_h, dnext_c = dgradmtx(h0, dnext_c, dprev_h, cache[j][3], cache[j][-5], cache[j][-4],  cache[j][-3], cache[j][-2], cache[j][-1]) \n",
    "        losses[i][j] = cache[j][0].T @ dgrad\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1031,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0\n",
      "component 0 - 0.09175951479805658\n",
      "Loss 1\n",
      "component 0 - 0.328342993531914\n",
      "component 1 - 0.6867388562834217\n",
      "Loss 2\n",
      "component 0 - 0.009819210665276431\n",
      "component 1 - 0.020492385142131097\n",
      "component 2 - 0.0619135094068817\n",
      "Loss 3\n",
      "component 0 - 0.006802684389668158\n",
      "component 1 - 0.014168432002208587\n",
      "component 2 - 0.017534503067068104\n",
      "component 3 - 0.12003729748087132\n",
      "Loss 4\n",
      "component 0 - 0.006551897919792712\n",
      "component 1 - 0.013646273537779396\n",
      "component 2 - 0.016985547973573215\n",
      "component 3 - 0.11453924637529664\n",
      "component 4 - 0.13527011296301064\n",
      "Loss 5\n",
      "component 0 - 0.015894562054862275\n",
      "component 1 - 0.03310522991818711\n",
      "component 2 - 0.041253705344984154\n",
      "component 3 - 0.27744942691921176\n",
      "component 4 - 0.33559773971184714\n",
      "component 5 - 0.22321271592497258\n",
      "Loss 6\n",
      "component 0 - 0.0031561049940361874\n",
      "component 1 - 0.006573542461387582\n",
      "component 2 - 0.008191440669589328\n",
      "component 3 - 0.055092644507990986\n",
      "component 4 - 0.06653431204983895\n",
      "component 5 - 0.04330085465999622\n",
      "component 6 - 0.1479228661017821\n",
      "Loss 7\n",
      "component 0 - 4.1207767035850004e-05\n",
      "component 1 - 8.582762993153105e-05\n",
      "component 2 - 0.00010695268578233199\n",
      "component 3 - 0.0007193107773405135\n",
      "component 4 - 0.0008696231927190596\n",
      "component 5 - 0.0005728399666797992\n",
      "component 6 - 0.002338641323972674\n",
      "component 7 - 0.06905741258689194\n",
      "Loss 8\n",
      "component 0 - 0.0005180528273272086\n",
      "component 1 - 0.0010790015814326505\n",
      "component 2 - 0.0013445723935744428\n",
      "component 3 - 0.0090430433444473\n",
      "component 4 - 0.010925091362694905\n",
      "component 5 - 0.0071302931520891765\n",
      "component 6 - 0.008955103503802395\n",
      "component 7 - 0.17198293890164282\n",
      "component 8 - 0.13975122550890545\n",
      "Loss 9\n",
      "component 0 - 0.0006417465301905864\n",
      "component 1 - 0.0013366311008002813\n",
      "component 2 - 0.0016656112469285206\n",
      "component 3 - 0.011202220743111585\n",
      "component 4 - 0.013533538215064601\n",
      "component 5 - 0.008832068635615265\n",
      "component 6 - 0.011371396059967691\n",
      "component 7 - 0.20884643235547384\n",
      "component 8 - 0.14761629650376684\n",
      "component 9 - 0.5481110302993656\n"
     ]
    }
   ],
   "source": [
    "for l in losses:\n",
    "    print(f\"Loss {l}\")\n",
    "    for ii, e in losses[l].items():\n",
    "        print(f\"component {ii} - {np.linalg.norm(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see for loss 9, component 0 has very low magnitude - it says how x1 affects L9 which is small. However, it should be less vanishing than for RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM longer sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1022,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "N, D, T, H = 1, 3, 10, 1\n",
    "\n",
    "x = np.random.randn(N, T, D)\n",
    "h0 = np.random.randn(N, H)\n",
    "Wx = np.random.randn(D, 4 * H)\n",
    "Wh = np.random.randn(H, 4 * H)\n",
    "b = np.random.randn(4 * H)\n",
    "\n",
    "out, cache = lstm_forward(x, h0, Wx, Wh, b)\n",
    "\n",
    "dout = np.random.randn(*out.shape)\n",
    "\n",
    "dx, dh0, dWx, dWh, db = lstm_backward(dout, cache)\n",
    "    \n",
    "dh = dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (x, next_h, prev_h, prev_c, Wx, Wh, next_h, next_c_t, i, f, o ,g) = cache\n",
    "losses = {i : {x_comp : 0 for x_comp in range(i)} for i in range(T)}\n",
    "for i in range(T-1, -1, -1):\n",
    "    dnext_c = np.zeros((h0.shape))\n",
    "    dprev_h =  dout[:, i, :]\n",
    "    for j in range(i, -1, -1):\n",
    "        dgrad, dprev_h, dnext_c = dgradmtx(h0, dnext_c, dprev_h, cache[j][3], cache[j][-5], cache[j][-4],  cache[j][-3], cache[j][-2], cache[j][-1]) \n",
    "        losses[i][j] = cache[j][0].T @ dgrad\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Captioning Model\n",
    "\n",
    "Now that you have implemented an LSTM, update the implementation of the `loss` method of the `CaptioningRNN` class in the file `cs231n/classifiers/rnn.py` to handle the case where `self.cell_type` is `lstm`. This should require adding less than 10 lines of code.\n",
    "\n",
    "Once you have done so, run the following to check your implementation. You should see a difference on the order of `e-10` or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  9.82445935443226\n",
      "expected loss:  9.82445935443\n",
      "difference:  2.261302256556519e-12\n"
     ]
    }
   ],
   "source": [
    "N, D, W, H = 10, 20, 30, 40\n",
    "word_to_idx = {'<NULL>': 0, 'cat': 2, 'dog': 3}\n",
    "V = len(word_to_idx)\n",
    "T = 13\n",
    "\n",
    "model = CaptioningRNN(\n",
    "    word_to_idx,\n",
    "    input_dim=D,\n",
    "    wordvec_dim=W,\n",
    "    hidden_dim=H,\n",
    "    cell_type='lstm',\n",
    "    dtype=np.float64\n",
    ")\n",
    "\n",
    "# Set all model parameters to fixed values\n",
    "for k, v in model.params.items():\n",
    "    model.params[k] = np.linspace(-1.4, 1.3, num=v.size).reshape(*v.shape)\n",
    "\n",
    "features = np.linspace(-0.5, 1.7, num=N*D).reshape(N, D)\n",
    "captions = (np.arange(N * T) % V).reshape(N, T)\n",
    "\n",
    "loss, grads = model.loss(features, captions)\n",
    "expected_loss = 9.82445935443\n",
    "\n",
    "print('loss: ', loss)\n",
    "print('expected loss: ', expected_loss)\n",
    "print('difference: ', abs(loss - expected_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfit LSTM Captioning Model on Small Data\n",
    "Run the following to overfit an LSTM captioning model on the same small dataset as we used for the RNN previously. You should see a final loss less than 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base dir  C:\\Users\\alexe\\Desktop\\stanford_CV\\assignment3_git\\cs231n\\datasets/coco_captioning\n",
      "(Iteration 1 / 100) loss: 79.551150\n",
      "(Iteration 11 / 100) loss: 43.829098\n",
      "(Iteration 21 / 100) loss: 30.062609\n",
      "(Iteration 31 / 100) loss: 14.020146\n",
      "(Iteration 41 / 100) loss: 6.006055\n",
      "(Iteration 51 / 100) loss: 1.857125\n",
      "(Iteration 61 / 100) loss: 0.639504\n",
      "(Iteration 71 / 100) loss: 0.283837\n",
      "(Iteration 81 / 100) loss: 0.234638\n",
      "(Iteration 91 / 100) loss: 0.121968\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "\n",
    "small_data = load_coco_data(max_train=50)\n",
    "\n",
    "small_lstm_model = CaptioningRNN(\n",
    "    cell_type='lstm',\n",
    "    word_to_idx=small_data['word_to_idx'],\n",
    "    input_dim=small_data['train_features'].shape[1],\n",
    "    hidden_dim=512,\n",
    "    wordvec_dim=256,\n",
    "    dtype=np.float32,\n",
    ")\n",
    "\n",
    "small_lstm_solver = CaptioningSolver(\n",
    "    small_lstm_model, small_data,\n",
    "    update_rule='adam',\n",
    "    num_epochs=50,\n",
    "    batch_size=25,\n",
    "    optim_config={\n",
    "     'learning_rate': 5e-3,\n",
    "    },\n",
    "    lr_decay=0.995,\n",
    "    verbose=True, print_every=10,\n",
    ")\n",
    "\n",
    "small_lstm_solver.train()\n",
    "\n",
    "# Plot the training losses\n",
    "plt.plot(small_lstm_solver.loss_history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training loss history')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print final training loss. You should see a final loss of less than 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "lstm_final_training_loss"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss:  0.08004410260627541\n"
     ]
    }
   ],
   "source": [
    "print('Final loss: ', small_lstm_solver.loss_history[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Sampling at Test Time\n",
    "Modify the `sample` method of the `CaptioningRNN` class to handle the case where `self.cell_type` is `lstm`. This should take fewer than 10 lines of code.\n",
    "\n",
    "When you are done run the following to sample from your overfit LSTM model on some training and validation set samples. As with the RNN, training results should be very good, and validation results probably won't make a lot of sense (because we're overfitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you get an error, the URL just no longer exists, so don't worry!\n",
    "# You can re-sample as many times as you want.\n",
    "for split in ['train', 'val']:\n",
    "    minibatch = sample_coco_minibatch(small_data, split=split, batch_size=2)\n",
    "    gt_captions, features, urls = minibatch\n",
    "    gt_captions = decode_captions(gt_captions, data['idx_to_word'])\n",
    "\n",
    "    sample_captions = small_lstm_model.sample(features)\n",
    "    sample_captions = decode_captions(sample_captions, data['idx_to_word'])\n",
    "\n",
    "    for gt_caption, sample_caption, url in zip(gt_captions, sample_captions, urls):\n",
    "        img = image_from_url(url)\n",
    "        # Skip missing URLs.\n",
    "        if img is None: continue\n",
    "        plt.imshow(img) \n",
    "        plt.title('%s\\n%s\\nGT:%s' % (split, sample_caption, gt_caption))\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
